{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "The most important type in DALI is the Pipeline. It contains all the necessary information and multiple functions related to defining, building, and running the pipeline.\n",
    "\n",
    "In order to make our own input and augmentation pipeline, we will make subclasses of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-data/images\n",
      "  |-file_list.txt\n",
      "  |-data/images/kitten\n",
      "    |-cat_5.jpg\n",
      "    |-cat_4.jpg\n",
      "    |-cat_8.jpg\n",
      "    |-cat_7.jpg\n",
      "    |-cat_2.jpg\n",
      "    |-cat_6.jpg\n",
      "    |-cat_3.jpg\n",
      "    |-cat_1.jpg\n",
      "    |-cat_10.jpg\n",
      "    |-cat_9.jpg\n",
      "  |-data/images/dog\n",
      "    |-dog_10.jpg\n",
      "    |-dog_1.jpg\n",
      "    |-dog_2.jpg\n",
      "    |-dog_4.jpg\n",
      "    |-dog_11.jpg\n",
      "    |-dog_5.jpg\n",
      "    |-dog_6.jpg\n",
      "    |-dog_8.jpg\n",
      "    |-dog_3.jpg\n",
      "    |-dog_7.jpg\n",
      "    |-dog_9.jpg\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import fnmatch\n",
    "\n",
    "for root, dir, files in os.walk(\"data/images\"):\n",
    "        depth = root.count('/')\n",
    "        ret = \"\"\n",
    "        if depth > 0:\n",
    "            ret += \"  \" * (depth - 1) + \"|-\"\n",
    "        print (ret + root)\n",
    "        for items in fnmatch.filter(files, \"*\"):\n",
    "                print (\" \" * len(ret) + \"|-\" + items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Pipeline\n",
    "Let's define a very simple pipeline for a classifier determining whether a picture contains a dog or cat.\n",
    "\n",
    "Our simple pipeline will read images from this directory, decode them, and return (image, label) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvidia.dali.ops as ops\n",
    "import nvidia.dali.types as types\n",
    "\n",
    "image_dir  = 'data/images'\n",
    "batch_size = 8\n",
    "\n",
    "class SimplePipeline(Pipeline):\n",
    "    def __init__(self, batch_size, num_threads, device_id):\n",
    "        super().__init__(batch_size, num_threads, device_id, seed=12)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   PIPELINE HELP   ###\n",
    "\n",
    "#from nvidia.dali.pipeline import Pipeline\n",
    "#help(Pipeline)\n",
    "\"\"\"\n",
    "Help on class Pipeline in module nvidia.dali.pipeline:\n",
    "\n",
    "class Pipeline(builtins.object)\n",
    " |  Pipeline(batch_size=-1, num_threads=-1, device_id=-1, seed=-1, \n",
    " |           exec_pipelined=True, prefetch_queue_depth=2, \n",
    " |           exec_async=True, bytes_per_sample=0, set_affinity=False, \n",
    " |           max_streams=-1, default_cuda_stream_priority=0, *, \n",
    " |            enable_memory_stats=False)\n",
    " |  \n",
    " |  Pipeline class is the base of all DALI data pipelines. The pipeline\n",
    " |  encapsulates the data processing graph and the execution engine.\n",
    " |  \n",
    " |  Parameters\n",
    " |  ----------\n",
    " |  `batch_size` : int, optional, default = -1\n",
    " |      Batch size of the pipeline. Negative values for this parameter\n",
    " |      are invalid - the default value may only be used with\n",
    " |      serialized pipeline (the value stored in serialized pipeline\n",
    " |      is used instead).\n",
    " |\n",
    " |  `num_threads` : int, optional, default = -1\n",
    " |      Number of CPU threads used by the pipeline.\n",
    " |      Negative values for this parameter are invalid - the default\n",
    " |      value may only be used with serialized pipeline (the value\n",
    " |      stored in serialized pipeline is used instead).\n",
    " |\n",
    " |  `device_id` : int, optional, default = -1\n",
    " |      Id of GPU used by the pipeline.\n",
    " |      Negative values for this parameter are invalid - the default\n",
    " |      value may only be used with serialized pipeline (the value\n",
    " |      stored in serialized pipeline is used instead).\n",
    " |\n",
    " |  `seed` : int, optional, default = -1\n",
    " |      Seed used for random number generation. Leaving the default value\n",
    " |      for this parameter results in random seed.\n",
    " |\n",
    " |  `exec_pipelined` : bool, optional, default = True\n",
    " |      Whether to execute the pipeline in a way that enables\n",
    " |      overlapping CPU and GPU computation, typically resulting\n",
    " |      in faster execution speed, but larger memory consumption.\n",
    " |\n",
    " |  `prefetch_queue_depth` : int or {\"cpu_size\": int, \"gpu_size\": int}, optional, default = 2\n",
    " |      Depth of the executor pipeline. Deeper pipeline makes DALI\n",
    " |      more resistant to uneven execution time of each batch, but it\n",
    " |      also consumes more memory for internal buffers.\n",
    " |      Specifying a dict:\n",
    " |      ``{ \"cpu_size\": x, \"gpu_size\": y }``\n",
    " |      instead of an integer will cause the pipeline to use separated\n",
    " |      queues executor, with buffer queue size `x` for cpu stage\n",
    " |      and `y` for mixed and gpu stages. It is not supported when both `exec_async`\n",
    " |      and `exec_pipelined` are set to `False`.\n",
    " |      Executor will buffer cpu and gpu stages separatelly,\n",
    " |      and will fill the buffer queues when the first :meth:`run`\n",
    " |      is issued.\n",
    " |\n",
    " |  `exec_async` : bool, optional, default = True\n",
    " |      Whether to execute the pipeline asynchronously.\n",
    " |      This makes :meth:`run` method\n",
    " |      run asynchronously with respect to the calling Python thread.\n",
    " |      In order to synchronize with the pipeline one needs to call\n",
    " |      :meth:`outputs` method.\n",
    " |\n",
    " |  `bytes_per_sample` : int, optional, default = 0\n",
    " |      A hint for DALI for how much memory to use for its tensors.\n",
    " |\n",
    " |  `set_affinity` : bool, optional, default = False\n",
    " |      Whether to set CPU core affinity to the one closest to the\n",
    " |      GPU being used.\n",
    " |\n",
    " |  `max_streams` : int, optional, default = -1\n",
    " |      Limit the number of CUDA streams used by the executor.\n",
    " |      Value of -1 does not impose a limit.\n",
    " |      This parameter is currently unused (and behavior of\n",
    " |      unrestricted number of streams is assumed).\n",
    " |\n",
    " |  `default_cuda_stream_priority` : int, optional, default = 0\n",
    " |      CUDA stream priority used by DALI. See `cudaStreamCreateWithPriority` in CUDA documentation\n",
    " |\n",
    " |  `enable_memory_stats`: bool, optional, default = False\n",
    " |      If DALI should print operator output buffer statistics.\n",
    " |      Usefull for `bytes_per_sample_hint` operator parameter.\n",
    " |  \n",
    " |  Methods defined here:\n",
    " |  \n",
    " |  __enter__(self)\n",
    " |      Safely sets the pipeline as current.\n",
    " |      Current pipeline is required to call operators with side effects or without outputs.\n",
    " |      Examples of such operators are `PythonFunction` (potential side effects) or `DumpImage`\n",
    " |      (no output).\n",
    " |      \n",
    " |      Any dangling operator can be marked as having side effects if it's marked\n",
    " |      with `preserve=True`, which can be useful for debugging - otherwise operator which\n",
    " |      does not contribute to the pipeline output is removed from the graph.\n",
    " |      \n",
    " |      To manually set new (and restore previous) current pipeline, use :meth:`push_current`\n",
    " |      and :meth:`pop_current`, respectively.\n",
    " |  \n",
    " |  __exit__(self, exception_type, exception_value, traceback)\n",
    " |      Safely restores previous pipeline.\n",
    " |  \n",
    " |  __init__(self, batch_size=-1, num_threads=-1, device_id=-1, seed=-1, exec_pipelined=True, prefetch_queue_depth=2, exec_async=True, bytes_per_sample=0, set_affinity=False, max_streams=-1, default_cuda_stream_priority=0, *, enable_memory_stats=False)\n",
    " |      Initialize self.  See help(type(self)) for accurate signature.\n",
    " |  \n",
    " |  add_sink(self, edge)\n",
    " |      Allows to manual add of graph edges to the pipeline which are not connected to the output and all pruned\n",
    " |  \n",
    " |  build(self, define_graph=None)\n",
    " |      Build the pipeline.\n",
    " |      \n",
    " |      Pipeline needs to be built in order to run it standalone.\n",
    " |      Framework-specific plugins handle this step automatically.\n",
    " |      \n",
    " |      Parameters\n",
    " |      ----------\n",
    " |      define_graph : callable\n",
    " |          If specified, this function will be used instead of member :meth:`define_graph`.\n",
    " |          This parameter must not be set, if the pipeline outputs are specified with\n",
    " |          :meth:`set_outputs`.\n",
    " |  \n",
    " |  define_graph(self)\n",
    " |      This function is defined by the user to construct the\n",
    " |      graph of operations for their pipeline.\n",
    " |      \n",
    " |      It returns a list of outputs created by calling DALI Operators.\n",
    " |  \n",
    " |  deserialize_and_build(self, serialized_pipeline)\n",
    " |      Deserialize and build the pipeline given in serialized form.\n",
    " |      \n",
    " |      Parameters\n",
    " |      ----------\n",
    " |      serialized_pipeline : str\n",
    " |                            Serialized pipeline.\n",
    " |  \n",
    " |  empty(self)\n",
    " |      If there is any work scheduled in the pipeline but not yet consumed\n",
    " |  \n",
    " |  enable_api_check(self, enable)\n",
    " |      Allows to enable or disable API check in the runtime\n",
    " |  \n",
    " |  epoch_size(self, name=None)\n",
    " |      Epoch size of a pipeline.\n",
    " |      \n",
    " |      If the `name` parameter is `None`, returns a dictionary of pairs\n",
    " |      `(reader name, epoch size for that reader)`.\n",
    " |      If the `name` parameter is not `None`, returns epoch size for that\n",
    " |      reader.\n",
    " |      \n",
    " |      Parameters\n",
    " |      ----------\n",
    " |      name : str, optional, default = None\n",
    " |          The reader which should be used to obtain epoch size.\n",
    " |  \n",
    " |  executor_statistics(self)\n",
    " |      Returns provided pipeline executor statistics metadata as a dictionary.\n",
    " |      Each key in the dictionary is the operator name. To enable it use ``executor_statistics``\n",
    " |      \n",
    " |      Available metadata keys for each operator:\n",
    " |      \n",
    " |      ``real_memory_size``:     list of memory sizes that is used by each output of the operator;\n",
    " |                                index in the list corresponds to the output index\n",
    " |      \n",
    " |      ``max_real_memory_size``: list of maximum tensor size that is used by each output of the operator;\n",
    " |                                index in the list corresponds to the output index\n",
    " |      \n",
    " |      ``reserved_memory_size``: list of memory sizes that is reserved for each of the operator outputs\n",
    " |                                index in the list corresponds to the output index\n",
    " |      \n",
    " |      ``max_reserved_memory_size``: list of maximum memory sizes per tensor that is reserved for each of the operator outputs\n",
    " |                                index in the list corresponds to the output index\n",
    " |  \n",
    " |  feed_input(self, data_node, data, layout='', cuda_stream=None, use_copy_kernel=False)\n",
    " |      Pass a mutlidimensional array or DLPack (or a list thereof) to an output of ExternalSource.\n",
    " |      In the case of the GPU input, the data must be modified on the same stream as the one\n",
    " |      used by feed_input. See ``cuda_stream`` parameter for details.\n",
    " |      \n",
    " |      Parameters\n",
    " |      ----------\n",
    " |      data_node : :class:`DataNode` or str\n",
    " |          The name of the :class:`nvidia.dali.ops.ExternalSource` node or a :class:`DataNode`\n",
    " |          object returned by a call to that ExternalSource.\n",
    " |      \n",
    " |      data : an ndarray or DLPack or a list thereof\n",
    " |          The array(s) may be one of:\n",
    " |            * NumPy ndarray (CPU)\n",
    " |            * MXNet ndarray (CPU)\n",
    " |            * PyTorch tensor (CPU or GPU)\n",
    " |            * CuPy array (GPU)\n",
    " |            * objects implementing ``__cuda_array_interface__``\n",
    " |      \n",
    " |          The data to be used as the output of the ExternalSource referred to by `data_node`.\n",
    " |      \n",
    " |      layout : str\n",
    " |          The description of the data layout (or empty string, if not specified).\n",
    " |          It should be a string of the length that matches the dimensionality of the data, batch\n",
    " |          dimension excluded. For a batch of channel-first images, this should be \"CHW\", for\n",
    " |          channel-last video it's \"FHWC\" and so on.\n",
    " |      \n",
    " |      cuda_stream : optional, `cudaStream_t` or an object convertible to `cudaStream_t`, e.g. `cupy.cuda.Stream`, `torch.cuda.Stream`\n",
    " |          The CUDA stream, which is going to be used for copying data to GPU or from a GPU\n",
    " |          source. If not set, best effort will be taken to maintain correctness - i.e. if the data\n",
    " |          is provided as a tensor/array from a recognized library (CuPy, PyTorch), the library's\n",
    " |          current stream is used. This should work in typical scenarios, but advanced use cases\n",
    " |          (and code using unsupported libraries) may still need to supply the stream handle\n",
    " |          explicitly.\n",
    " |      \n",
    " |          Special values:\n",
    " |            *  0 - use default CUDA stream\n",
    " |            * -1 - use DALI's internal stream\n",
    " |      \n",
    " |          If internal stream is used, the call to ``feed_input`` will block until the copy to\n",
    " |          internal buffer is complete, since there's no way to synchronize with this stream to\n",
    " |          prevent overwriting the array with new data in another stream.\n",
    " |      \n",
    " |      use_copy_kernel : optional, `bool`\n",
    " |          If set to True, DALI will use a CUDA kernel to feed the data (only applicable when copying\n",
    " |          data to/from GPU memory) instead of cudaMemcpyAsync (default).\n",
    " |  \n",
    " |  iter_setup(self)\n",
    " |      This function can be overriden by user-defined\n",
    " |      pipeline to perform any needed setup for each iteration.\n",
    " |      For example, one can use this function to feed the input\n",
    " |      data from NumPy arrays.\n",
    " |  \n",
    " |  outputs(self)\n",
    " |      Returns the outputs of the pipeline and releases previous buffer.\n",
    " |      \n",
    " |      If the pipeline is executed asynchronously, this function blocks\n",
    " |      until the results become available. It rises StopIteration if data set\n",
    " |      reached its end - usually when iter_setup cannot produce any more data.\n",
    " |      \n",
    " |      :return:\n",
    " |          A list of `TensorList` objects for respective pipeline outputs\n",
    " |  \n",
    " |  reader_meta(self, name=None)\n",
    " |      Returns provided reader metadata as a dictionary. If no name is provided if provides\n",
    " |      a dictionary with data for all readers as {reader_name : meta}\n",
    " |      \n",
    " |      Available metadata keys:\n",
    " |      \n",
    " |      ``epoch_size``:        raw epoch size\n",
    " |      \n",
    " |      ``epoch_size_padded``: epoch size with the padding at the end to be divisible by the number of shards\n",
    " |      \n",
    " |      ``number_of_shards``:  number of shards\n",
    " |      \n",
    " |      ``shard_id``:          shard id of given reader\n",
    " |      \n",
    " |      ``pad_last_batch``:    if given reader should pad last batch\n",
    " |      \n",
    " |      ``stick_to_shard``:    if given reader should stick to its shard\n",
    " |      \n",
    " |      Parameters\n",
    " |      ----------\n",
    " |      name : str, optional, default = None\n",
    " |          The reader which should be used to obtain shards_number.\n",
    " |  \n",
    " |           default_cuda_stream_priority=0, *, enable_memory_stats=False)\n",
    " |  release_outputs(self)\n",
    " |      Release buffers returned by share_outputs calls.\n",
    " |      \n",
    " |      It helps in case when output call result is consumed (copied)\n",
    " |      and buffers can be marked as free before the next call to share_outputs. It provides\n",
    " |      the user with better control about when he wants to run the pipeline, when he wants\n",
    " |      to obtain the resulting buffers and when they can be returned to DALI pool when the\n",
    " |      results have been consumed.\n",
    " |      Needs to be used together with :meth:`schedule_run`\n",
    " |      and :meth:`share_outputs`\n",
    " |      Should not be mixed with :meth:`run` in the same pipeline\n",
    " |  \n",
    " |  reset(self)\n",
    " |      Resets pipeline iterator\n",
    " |      \n",
    " |      If pipeline iterator reached the end then reset its state to the beginning.\n",
    " |  \n",
    " |  run(self)\n",
    " |      Run the pipeline and return the result.\n",
    " |      \n",
    " |      If the pipeline was created with `exec_pipelined` option set to `True`,\n",
    " |      this function will also start prefetching the next iteration for\n",
    " |      faster execution.\n",
    " |      Should not be mixed with :meth:`schedule_run` in the same pipeline,\n",
    " |      :meth:`share_outputs` and\n",
    " |      :meth:`release_outputs`\n",
    " |      \n",
    " |      :return:\n",
    " |          A list of `TensorList` objects for respective pipeline outputs\n",
    " |  \n",
    " |  save_graph_to_dot_file(self, filename, show_tensors=False, show_ids=False, use_colors=False)\n",
    " |      Saves the pipeline graph to a file.\n",
    " |      \n",
    " |      Parameters\n",
    " |      ----------\n",
    " |      filename : str\n",
    " |                 Name of the file to which the graph is written.\n",
    " |      show_tensors : bool\n",
    " |                 Show the Tensor nodes in the graph (by default only Operator nodes are shown)\n",
    " |      show_ids : bool\n",
    " |                 Add the node id to the graph representation\n",
    " |      use_colors : bool\n",
    " |                 Whether use color to distinguish stages\n",
    " |  \n",
    " |  schedule_run(self)\n",
    " |      Run the pipeline without returning the resulting buffers.\n",
    " |      \n",
    " |      If the pipeline was created with `exec_pipelined` option set to `True`,\n",
    " |      this function will also start prefetching the next iteration for\n",
    " |      faster execution. It provides better control to the users about when they\n",
    " |      want to run the pipeline, when they want to obtain resulting buffers\n",
    " |      and return them to DALI buffer pool when the results have been consumed.\n",
    " |      Needs to be used together with :meth:`release_outputs`\n",
    " |      and :meth:`share_outputs`.\n",
    " |      Should not be mixed with :meth:`run` in the same pipeline\n",
    " |  \n",
    " |  serialize(self, define_graph=None, filename=None)\n",
    " |      Serialize the pipeline to a Protobuf string.\n",
    " |      \n",
    " |      Additionally, you can pass file name, so that serialized pipeline will be written there.\n",
    " |      The file contents will be overwritten\n",
    " |      \n",
    " |      Parameters\n",
    " |      ----------\n",
    " |      define_graph : allable\n",
    " |              If specified, this function will be used instead of member :meth:`define_graph`.\n",
    " |              This parameter must not be set, if the pipeline outputs are specified with\n",
    " |              :meth:`set_outputs`.\n",
    " |      filename : str\n",
    " |              File, from where serialized pipeline will be writeen.\n",
    " |      kwargs : dict\n",
    " |              Refer to Pipeline constructor for full list of arguments.\n",
    " |  \n",
    " |  set_outputs(self, *output_data_nodes)\n",
    " |      Set the outputs of the pipeline.\n",
    " |      \n",
    " |      Use of this function is an alternative to overriding `define_graph` in a derived class.\n",
    " |      \n",
    " |      Args\n",
    " |      ----\n",
    " |      `*output_data_nodes` : unpacked list of :class:`DataNode` objects\n",
    " |          The outputs of the pipeline\n",
    " |  \n",
    " |  share_outputs(self)\n",
    " |      Returns the outputs of the pipeline.\n",
    " |      \n",
    " |      Main difference to :meth:`outputs`\n",
    " |      is that share_outputs doesn't release returned buffers, release_outputs\n",
    " |      need to be called for that. If the pipeline is executed asynchronously,\n",
    " |      this function blocks until the results become available. It provides\n",
    " |      the user with better control about when he wants to run the pipeline, when he wants\n",
    " |      to obtain the resulting buffers and when they can be returned to DALI pool when the\n",
    " |      results have been consumed.\n",
    " |      Needs to be used together with :meth:`release_outputs`\n",
    " |      and :meth:`schedule_run`\n",
    " |      Should not be mixed with :meth:`run` in the same pipeline.\n",
    " |      \n",
    " |      :return:\n",
    " |          A list of `TensorList` objects for respective pipeline outputs\n",
    " |  \n",
    " |  ----------------------------------------------------------------------\n",
    " |  Class methods defined here:\n",
    " |  \n",
    " |  deserialize(serialized_pipeline=None, filename=None, **kwargs) from builtins.type\n",
    " |      Deserialize and build pipeline.\n",
    " |      \n",
    " |      Deserialize pipeline, previously serialized with ``serialize()`` method.\n",
    " |      \n",
    " |      Returned pipeline is already built.\n",
    " |      \n",
    " |      Alternatively, additional arguments can be passed, which will be used when instantiating\n",
    " |      the pipeline. Refer to Pipeline constructor for full list of arguments. By default,\n",
    " |      the pipeline will be instantiated with the arguments from serialized pipeline.\n",
    " |      \n",
    " |      Note, that ``serialized_pipeline`` and ``filename`` parameters are mutually exclusive\n",
    " |      \n",
    " |      Parameters\n",
    " |      ----------\n",
    " |      serialized_pipeline : str\n",
    " |                 Pipeline, serialized using ``serialize()`` method.\n",
    " |      filename : str\n",
    " |                 File, from which serialized pipeline will be read.\n",
    " |      kwargs : dict\n",
    " |                 Refer to Pipeline constructor for full list of arguments.\n",
    " |      \n",
    " |      Returns\n",
    " |      ----------\n",
    " |      Deserialized and built pipeline.\n",
    " |  \n",
    " |  ----------------------------------------------------------------------\n",
    " |  Static methods defined here:\n",
    " |  \n",
    " |  current()\n",
    " |  \n",
    " |  pop_current()\n",
    " |      Restores previous pipeline as current. Complementary to :meth:`push_current`.\n",
    " |  \n",
    " |  push_current(pipeline)\n",
    " |      Sets the pipeline as current and stores the previous current pipeline\n",
    " |      on stack. To restore previous pipeline as current, use :meth:`pop_current`.\n",
    " |      \n",
    " |      To make sure that the pipeline is properly restored in case of exception, use context\n",
    " |      manager (`with my_pipeline:`).\n",
    " |      \n",
    " |      Current pipeline is required to call operators with side effects or without outputs.\n",
    " |      Examples of such operators are `PythonFunction` (potential side effects) or `DumpImage`\n",
    " |      (no output).\n",
    " |      \n",
    " |      Any dangling operator can be marked as having side effects if it's marked\n",
    " |      with `preserve=True`, which can be useful for debugging - otherwise operator which\n",
    " |      does not contribute to the pipeline output is removed from the graph.\n",
    " |  \n",
    " |  ----------------------------------------------------------------------\n",
    " |  Readonly properties defined here:\n",
    " |  \n",
    " |  batch_size\n",
    " |      Batch size.\n",
    " |  \n",
    " |  device_id\n",
    " |      Id of the GPU used by the pipeline.\n",
    " |  \n",
    " |  exec_async\n",
    " |  \n",
    " |  exec_pipelined\n",
    " |  \n",
    " |  num_threads\n",
    " |      Number of CPU threads used by the pipeline.\n",
    " |  \n",
    " |  ----------------------------------------------------------------------\n",
    " |  Data descriptors defined here:\n",
    " |  \n",
    " |  __dict__\n",
    " |      dictionary for instance variables (if defined)\n",
    " |  \n",
    " |  __weakref__\n",
    " |      list of weak references to the object (if defined)\n",
    "\n",
    "\"\"\"\n",
    "noprint = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
